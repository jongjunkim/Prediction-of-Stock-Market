{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jongjunkim/Prediction-of-Stock-Market/blob/main/CS435_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swKZntiiuoh_",
        "outputId": "bf0fdc6f-1315-400f-be34-c56cf0fb40db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdeBFrQPvei3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08b190a-2260-4d01-87a6-4286cc70c936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/CS435_Project/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/CS435_Project/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B03iLMgBmjsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcc7b12-4166-4903-809b-725a557f3fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbQc_uqLmb1c",
        "outputId": "3c0d03ce-b4a5-4824-dbe5-80d11ae7bc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.17)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (40.0.1)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXtPHKBaw2Mc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import yfinance as yf\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaqEhQUixf5X"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/CS435_Project/training_set.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/CS435_Project/training_set.pkl', 'rb') as t1:\n",
        "        test1 = pickle.load(t1)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/CS435_Project/training_set.pkl', 'rb') as t2:\n",
        "        test2 = pickle.load(t2)\n",
        "\n",
        "original_data = data\n",
        "#copy from original data so that it doesn't modify the original data\n",
        "data_frame = copy.deepcopy(original_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1l8LhGMyxdC"
      },
      "source": [
        "#Task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3bsodgbxnPt"
      },
      "outputs": [],
      "source": [
        "#Calculate the daily percentage changes of the close prices for each stock and return the daily percentage changes \n",
        "def daily_percentage_change_for_close(data):\n",
        "    percentage_changes = []\n",
        "    for df in data:\n",
        "        close_prices = df['Close']\n",
        "        pct_changes = []\n",
        "        for i in range(1, len(close_prices)):\n",
        "            today_close = close_prices[i-1]\n",
        "            next_close = close_prices[i]\n",
        "            pct_change = ((next_close - today_close) / today_close) * 100\n",
        "            pct_changes.append(pct_change)\n",
        "        percentage_changes.append(pct_changes)\n",
        "    return percentage_changes\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HQzGCwKyOCP"
      },
      "outputs": [],
      "source": [
        "def divide_into_levels(percentage_changes, thresholds):\n",
        "    \"\"\"\n",
        "    Divides the daily percentage changes into three levels based on the provided thresholds.\n",
        "\n",
        "    Parameters:\n",
        "    percentage_changes (list of pandas.Series): The daily percentage changes for each stock.\n",
        "    thresholds (list of tuples): The two threshold values.\n",
        "\n",
        "    Returns:\n",
        "    list of lists: The level classifications for each day of each stock.\n",
        "    \"\"\"\n",
        "    levels = []\n",
        "    for i in range(len(percentage_changes)):\n",
        "        stock_levels = []\n",
        "        for j in range(len(percentage_changes[i])):\n",
        "            if percentage_changes[i][j] < thresholds[0][0]:\n",
        "                stock_levels.append(0)\n",
        "            elif percentage_changes[i][j] > thresholds[0][1]:\n",
        "                stock_levels.append(2)\n",
        "            else:\n",
        "                stock_levels.append(1)\n",
        "        levels.append(stock_levels)\n",
        "    return levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xpuG7pL0KX9"
      },
      "outputs": [],
      "source": [
        "def create_label_vectors(levels, labels):\n",
        "    \"\"\"\n",
        "    Converts levels into a 3-dimensional vector representation.\n",
        "\n",
        "    Parameters:\n",
        "    levels (list of lists): The levels for each stock.\n",
        "    labels (list of str): The labels corresponding to each level.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: The label vectors.\n",
        "    \"\"\"\n",
        "    num_classes = len(labels)\n",
        "\n",
        "    # Convert the levels to integer values\n",
        "    label_integers = []\n",
        "    for stock_levels in levels:\n",
        "        stock_integers = []\n",
        "        for level in stock_levels:\n",
        "            if level == 0:\n",
        "                stock_integers.append(0)\n",
        "            elif level == 1:\n",
        "                stock_integers.append(1)\n",
        "            elif level == 2:\n",
        "                stock_integers.append(2)\n",
        "        label_integers.append(stock_integers)\n",
        "\n",
        "    # Convert the integer labels to label vectors\n",
        "    label_vectors = np.zeros((len(label_integers), len(label_integers[0]), num_classes))\n",
        "    for i in range(len(label_integers)):\n",
        "        for j in range(len(label_integers[i])):\n",
        "            label_vectors[i, j, label_integers[i][j]] = 1\n",
        "\n",
        "    return label_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9PhyxxPyO22"
      },
      "outputs": [],
      "source": [
        "#print the two threshold used to divde data into three levels\n",
        "#add the number of data points for each level and print them\n",
        "def threshold_number_of_data_points(levels, thresholds):\n",
        "\n",
        "    increase = 0\n",
        "    decrease = 0\n",
        "    no_change = 0\n",
        "\n",
        "    for i in range(len(levels)):\n",
        "        increase += sum([1 for level in levels[i] if level == 2])\n",
        "        decrease += sum([1 for level in levels[i] if level == 0])\n",
        "        no_change += sum([1 for level in levels[i] if level == 1])\n",
        "\n",
        "    print(\"Threshold 1:\", thresholds[0][0])\n",
        "    print(\"Threshold 2:\", thresholds[0][1])\n",
        "    print(\"Number of increases:\", increase)\n",
        "    print(\"Number of decreases:\", decrease)\n",
        "    print(\"Number of no big changes:\", no_change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0-K9ljkyRYn"
      },
      "outputs": [],
      "source": [
        "#To get the two threshold, we use 33th and 66th percentile.\n",
        "#return list of thresholds   \n",
        "def find_thresholds(closed_percentage):\n",
        "\n",
        "    #combine 2000X2202 data int one dimesional data\n",
        "    combine_into_one = np.array(closed_percentage).ravel()\n",
        "    sorted_closed_percentage = np.sort(combine_into_one)\n",
        "    thresholds = []\n",
        "    threshold1 = np.nanpercentile(sorted_closed_percentage, 33.3)\n",
        "    threshold2 = np.nanpercentile(sorted_closed_percentage, 66.6)\n",
        "    thresholds.append((threshold1, threshold2))\n",
        "\n",
        "    return thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkI8pZi2zAHH"
      },
      "source": [
        "#Task2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sektbpny-ei"
      },
      "outputs": [],
      "source": [
        "#calculate a daily return which is calculated by substracting the opening price from the closing price\n",
        "def daily_return(data_frame):\n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['Daily_return'] = df.Close - df.Open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G9dK2-TzECU"
      },
      "outputs": [],
      "source": [
        "#daily log return\n",
        "def daily_log_return(data_frame):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['log_return'] = np.log(df['Close'] / df['Close'].shift(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlvvhIUKzELu"
      },
      "outputs": [],
      "source": [
        "def moving_average(data_frame, window_size):\n",
        "      \n",
        "    for i, df in enumerate(data_frame):\n",
        "        close_prices = df['Close']\n",
        "        moving_average = close_prices.rolling(window = window_size).mean()\n",
        "        data_frame[i][str(window_size) + '-day MA'] = moving_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0eZihWzEN1"
      },
      "outputs": [],
      "source": [
        "#calculate standard_devation\n",
        "def standard_deviation(data_frame, window_size):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        close_prices = df['Close']\n",
        "        std_dev = close_prices.rolling(window =window_size).std()\n",
        "        data_frame[i][str(window_size) + '-day S.D'] = std_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVT7vS4SzEP7"
      },
      "outputs": [],
      "source": [
        "def median_price(data_frame):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i]['median'] = (df.High + df.Low)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es5S2_EAzESB"
      },
      "outputs": [],
      "source": [
        "#calcualte volatility\n",
        "def volatility(data_frame, window_size = 10):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i]['volatility'] = abs(df['Close'] - df['Close'].shift(1)).rolling(window_size).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIec-IEHzEUL"
      },
      "outputs": [],
      "source": [
        "#Donchian_Channel consists of Lower Channel, Upper Channel, and Middle Channel\n",
        "#typically 20 days\n",
        "def donchian_channel(data_frame, window_size = 20):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['Higher_Channel'] = df.High.rolling(window_size).max()\n",
        "        data_frame[i]['Lower_Channel'] = df.Low.rolling(window_size).min()\n",
        "        data_frame[i]['Middle_Channel'] = (df.Higher_Channel + df.Lower_Channel) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOCiuR-QzEWQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#calculates upper and lower Bollinger Bands, using moving_averge and two standard deviation\n",
        "#upperband and lower band are the lines that are plotted at a distance of 'k'\n",
        "#typically window_size is 20 and k is usually 2\n",
        "#added Bollinger Bands Squeeze - BandWidth, Avg BandWidth, Low Volatility, Breakout, and Breakdown\n",
        "def bollinger_bands(data_frame, window_size, k = 2):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        close_prices = df['Close']\n",
        "        moving_average = close_prices.rolling(window_size).mean()\n",
        "        std_dev = close_prices.rolling(window_size).std()\n",
        "        upper_band = moving_average + k * std_dev\n",
        "        lower_band = moving_average - k * std_dev\n",
        "        data_frame[i][str(window_size) + '-day BOLU'] = upper_band\n",
        "        data_frame[i][str(window_size) + '-day BOLD'] = lower_band"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tescIw41zEaV"
      },
      "outputs": [],
      "source": [
        "#Expoenetial Moving Average that gives more weight to recent prices \n",
        "#short term tpyically (12 or 26 days) while Long term (50 day or 200 days) \n",
        "def ema(data_frame, window_size):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        exponetial = df.Close.ewm(span=window_size, adjust = False).mean()\n",
        "        data_frame[i][str(window_size) + '-day EMA'] = exponetial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywu5Yj7czEcO"
      },
      "outputs": [],
      "source": [
        "#Zero Lag Exponential Moving average(ZLEMA)\n",
        "def zero_lag_ema(data_frame, window_size = 5):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        lag = int((window_size-1)/2)\n",
        "        emadata = df.Close + (df.Close - df.Close.shift(lag))\n",
        "        data_frame[i]['ZLEMA'] = emadata.ewm(span = window_size, adjust = False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvrT8XjszEeA"
      },
      "outputs": [],
      "source": [
        "# Calculate weighted_moving_average that assigns higher weight to more recent values and lower weight to older values\n",
        "# 90 period weighted_moving_average\n",
        "def WMA(data_frames, window_size = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frames):\n",
        "        w = np.arange(1, window_size + 1, 1)\n",
        "        wma = df.Close.rolling(window_size).apply(lambda x: np.dot(x, w)/w.sum(), raw=True)\n",
        "        data_frames[i][str(window_size) + '-day WMA'] = wma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_98yKhizEgO"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Hull Moving Average(MMA)\n",
        "def Hull_MA(data_frame, window_size = 29):\n",
        "\n",
        "\n",
        "    def get_wma(data, period):\n",
        "        w = np.arange(1, window_size + 1, 1)\n",
        "        wma = df.Close.rolling(window_size).apply(lambda x: np.dot(x, w)/w.sum(), raw=True)\n",
        "        return wma\n",
        "    \n",
        "    for i,df in enumerate(data_frame):\n",
        "        wma1 = get_wma(df, window_size/2)\n",
        "        wma2 = get_wma(df, window_size)\n",
        "        raw_hma = (2 * wma1) - wma2\n",
        "        data_frame[i]['HMA'] = get_wma(raw_hma, math.sqrt(window_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVlHcoTkzEiC"
      },
      "outputs": [],
      "source": [
        "# Calculate Momentum that measures the strength of the price trend in an asset \n",
        "# typical momentum day is 14 days or 21 days\n",
        "def momentum(data_frames, window_size = 14):\n",
        "    \n",
        "    for i,df in enumerate(data_frames):\n",
        "        stock_momentum = df.Close.diff(window_size)\n",
        "        data_frames[i][str(window_size) + '-day Momentum'] = stock_momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2yInjX6zEki"
      },
      "outputs": [],
      "source": [
        "#Rate of Change\n",
        "#Measures the percentage change the current closing price and the closing price n-time days ago and ange\n",
        "#Standard calculation is 12, 25, 200\n",
        "def ROC(data_frames, window_size):\n",
        " \n",
        "    for i,df in enumerate(data_frames):\n",
        "        stock_roc = ((df.Close - df.Close.shift(window_size)) / df.Close.shift(window_size) * 100)\n",
        "        data_frames[i][str(window_size) + '-day ROC'] = stock_roc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzi4YeUmzU4h"
      },
      "outputs": [],
      "source": [
        "#Relative Strength Index = Dynamic Momentum Index(DMI)\n",
        "#Momentum indicator that compares the magnitude of recent gains to recent losses to determine overbought or oversold conditions in an asset.\n",
        "# typcially window size is 14 days\n",
        "def RSI(data_frames, window_size = 14):\n",
        "    \n",
        "   for i, df in enumerate(data_frames):\n",
        "        close_prices = df['Close']\n",
        "        diff = close_prices.diff()\n",
        "        up, down = diff.copy(), diff.copy()\n",
        "        up[up < 0] = 0\n",
        "        down[down > 0] = 0\n",
        "        gain = up.rolling(window_size).mean()\n",
        "        loss = down.abs().rolling(window_size).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100.0 - (100.0 / (1.0 + rs))\n",
        "        data_frames[i]['RSI'] = rsi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuyIRHNqzVAG"
      },
      "outputs": [],
      "source": [
        "#Stochastic Oscillator\n",
        "#momentum indicator that compares a security's close price to its price range over a specified number of days\n",
        "#if below 20% oversold, above 80% overbought\n",
        "#typically 14-day day\n",
        "def stochastic_oscillator(data_frames, window_size = 14):\n",
        "\n",
        "    for i,df in enumerate(data_frames):\n",
        "        close = df['Close']\n",
        "        low = df['Low'].rolling(window_size).min()\n",
        "        high = df['High'].rolling(window_size).max()\n",
        "        stochastic = (close - low) / (high - low) * 100\n",
        "        data_frames[i][str(window_size) + '-day Stoch'] = stochastic\n",
        "        slowStoch = stochastic.rolling(3).mean()\n",
        "        data_frames[i]['3_day Stoch'] = slowStoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Om62M6pzVCK"
      },
      "outputs": [],
      "source": [
        "#Awesome Oscillator(AO)\n",
        "#shows what's happening to the market driving force at the present moment\n",
        "def awesome_Oscillator(data_frames, short_period = 5, long_period = 34):\n",
        "\n",
        "    for i, df in enumerate(data_frames):\n",
        "        median_price = (df.High + df.Low)/2\n",
        "        data_frames[i]['AO'] = median_price.rolling(short_period).mean() - median_price.rolling(long_period).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ah0Tv2zVEA"
      },
      "outputs": [],
      "source": [
        "#Calculate Moving Average Convergence Divergence\n",
        "def MACD(data_frames, fast_day=12, slow_day=26, signal_day=9):\n",
        "    for i, df in enumerate(data_frames):\n",
        "        fast_ma = df.Close.ewm(span = fast_day, adjust = False).mean()\n",
        "        slow_ma = df.Close.ewm(span = slow_day, adjust = False).mean()\n",
        "        data_frames[i]['MACD'] = fast_ma - slow_ma\n",
        "        signal_line = df.MACD.ewm(span=signal_day, adjust = False).mean()\n",
        "        data_frames[i]['signal Line'] = signal_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvrYxZnLzZv9"
      },
      "outputs": [],
      "source": [
        "#Williams %R  \n",
        "#momentum oscillator that measures overbought and oversold levels \n",
        "#typically 14 days\n",
        "def williams_R(data_frames, lookback_day = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frames):\n",
        "        highest_high = df['High'].rolling(lookback_day).max()\n",
        "        lowest_low = df['Low'].rolling(lookback_day).min()\n",
        "        williams_R = (highest_high - df['Close']) / (highest_high - lowest_low) * -100\n",
        "        data_frames[i][str(lookback_day) + '-day William %R'] = williams_R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNtXRAF7zZ2R"
      },
      "outputs": [],
      "source": [
        "#Aroon indictor\n",
        "#determine if an stock is in a trend and how strong the trend is\n",
        "def aroon_indicator(data_frames, window_size = 25):\n",
        "\n",
        "    for i, df in enumerate(data_frames):\n",
        "        aroon_high = df['High'].rolling(min_periods = window_size, window = window_size, center = False).apply(np.argmax)\n",
        "        aroon_low = df['Low'].rolling(min_periods = window_size, window = window_size, center = False).apply(np.argmin)\n",
        "        aroon_up = (((window_size - (aroon_high + 1)) / window_size)) * 100 \n",
        "        aroon_down = (((window_size - (aroon_low + 1)) / window_size)) * 100 \n",
        "        data_frames[i][str(window_size) + '-day Aroon Up'] = aroon_up\n",
        "        data_frames[i][str(window_size) + '-day Aroon Down'] = aroon_down\n",
        "        data_frames[i]['Aroon Oscillator'] = aroon_up - aroon_down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroW-koMzZ7y"
      },
      "outputs": [],
      "source": [
        "#Pivot points (pivot point, Resistance1, Resistance2, Support1, Support2)\n",
        "def pivot_points(data_frame):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i]['pivot_point'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "        data_frame[i]['Resistance1'] = (df.pivot_point * 2) - df.Low\n",
        "        data_frame[i]['Resistance2'] = (df.pivot_point + (df.High - df.Low))\n",
        "        data_frame[i]['Support1'] = (df.pivot_point * 2) - df.High\n",
        "        data_frame[i]['Supprot2'] = (df.pivot_point -  (df.High - df.Low))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCw9EbUVzZ9s"
      },
      "outputs": [],
      "source": [
        "#Commodity Channel Index\n",
        "#relatively high when prices are far from average\n",
        "def CCI(data_frame, window_size = 20):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "        typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "        tp_avg = typical_price.rolling(window_size).mean()\n",
        "        mean_deviation =  abs(typical_price - tp_avg).rolling(window_size).mean()\n",
        "        cci = (typical_price - tp_avg) / (0.015 * mean_deviation)\n",
        "        data_frame[i][str(window_size)+ '-day CCI'] = cci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgtMnbA8zZ_x"
      },
      "outputs": [],
      "source": [
        "#Coppock curve to determine if it is bull market\n",
        "#Typciall applied roc14 is 14period(months) = 420 and roc11 is 11 period(months) = 330\n",
        "#10 months of weighted average\n",
        "def coppock_curve(data_frame, wma_period = 300, roc14_period = 420, roc11_period = 330):\n",
        "\n",
        "    def get_wma(data, window_size):\n",
        "        w = np.arange(1, window_size + 1, 1)\n",
        "        wma = data.rolling(window_size).apply(lambda x: np.dot(x, w)/w.sum(), raw=True)\n",
        "        return wma\n",
        "    \n",
        "    def get_roc(data, window_size):\n",
        "        stock_roc = ((data - data.shift(window_size)) / data.shift(window_size) * 100)\n",
        "        return stock_roc\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        long_roc = get_roc(df.Close, roc14_period)\n",
        "        short_roc = get_roc(df.Close, roc11_period)\n",
        "        ROC = long_roc + short_roc\n",
        "        coppock = get_wma(ROC, wma_period)\n",
        "        data_frame[i]['Coppock Curve'] = coppock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OqRx03jzaBg"
      },
      "outputs": [],
      "source": [
        "#On balance Volume(OBV) and On_balance_Volume_Mean\n",
        "def on_balance_volume(data_frame):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['OBV'] = (np.sign(df[\"Close\"].diff()) * df[\"Volume\"]).cumsum()\n",
        "        data_frame[i]['OBV_Mean'] = df.OBV.rolling(14).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N9sS115zaDa"
      },
      "outputs": [],
      "source": [
        "#Volume Oscillator(VO)\n",
        "def Volume_Oscillator(data_frame, short_window = 14, long_window =28):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        short_period = df.Volume.rolling(short_window).mean()\n",
        "        long_period = df.Volume.rolling(long_window).mean()\n",
        "        data_frame[i]['VO'] = ((short_period - long_period)/long_period) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMXCiqiqzVGK"
      },
      "outputs": [],
      "source": [
        "#Chaikin Money Flow(CMF) (MF = money flow)\n",
        "#typically 20 period\n",
        "def chaikin_money_flow(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['MF_Multiplier'] = ((df.Close - df.Low) - (df.High - df.Close)) / (df.High - df.Low)\n",
        "        data_frame[i]['MF_Volume'] = df.MF_Multiplier * df.Volume\n",
        "        data_frame[i]['CMF'] = df.MF_Volume.rolling(21).mean() / df.Volume.rolling(21).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRtNGS_azjSK"
      },
      "outputs": [],
      "source": [
        "#accumulation_distribution(ADI)\n",
        "#use relationship between the stock's price and volume flow\n",
        "def accumulation_distribution(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        data_frame[i]['ADI'] = df.MF_Volume.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I35BmdJozjT8"
      },
      "outputs": [],
      "source": [
        "#Fibonacci retracement levels(FRL)\n",
        "#The Fibanoacci Ratios are typically 0.236 0.382 0.618 \n",
        "def Fibonacci_retracement_levels(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        maximum = df.Close.max()\n",
        "        minimum = df.Close.min()\n",
        "        diff = maximum - minimum\n",
        "        data_frame[i]['FRL_lev1'] = maximum - 0.236 * diff\n",
        "        data_frame[i]['FRL_lev2'] = maximum - 0.382 * diff\n",
        "        data_frame[i]['FRL_lev3'] = maximum - 0.618 * diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySWzb3GyzjWN"
      },
      "outputs": [],
      "source": [
        "#Money Flow Index(MFI)\n",
        "#MFI above 80 considred overbought and below 20 oversold\n",
        "def money_flow_index(data_frame, period = 14):\n",
        "\n",
        "    def get_mfi(df, period = 14):\n",
        "        typical_price = (df['Close'] + df['High'] + df['Low']) / 3\n",
        "        money_flow = typical_price * df['Volume']\n",
        "        positive_flow = [money_flow[i-1] if typical_price[i] > typical_price[i-1] else 0 for i in range(1, len(typical_price))]\n",
        "        negative_flow = [money_flow[i-1] if typical_price[i] < typical_price[i-1] else 0 for i in range(1, len(typical_price))]\n",
        "        positive_mf = [sum(positive_flow[i + 1- period : i+1]) for i in range(period-1, len(positive_flow))]\n",
        "        negative_mf = [sum(negative_flow[i + 1- period : i+1]) for i in range(period-1, len(negative_flow))]\n",
        "        MFI = 100 * (np.array(positive_mf) / (np.array(positive_mf) + np.array(negative_mf)))\n",
        "        df.loc[period:, 'MF'] = money_flow\n",
        "        df.loc[period:, 'MFI'] = MFI\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        get_mfi(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs6QnCwxzjYR"
      },
      "outputs": [],
      "source": [
        "#the Triple Exponential Moving Average(TEMA or sometimes called TRIX)\n",
        "def TEMA(data_frame, window_size = 15):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        exponetial1 = df.Close.ewm(span=window_size, adjust = False).mean()\n",
        "        exponetial2 = exponetial1.ewm(span = window_size, adjust = False).mean()\n",
        "        exponetial3 = exponetial1.ewm(span = window_size, adjust = False).mean()\n",
        "        data_frame[i]['TEMA'] = (3*exponetial1) - (3*exponetial2) + exponetial3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoYw0xFdzjZ7"
      },
      "outputs": [],
      "source": [
        "#Volume Price Trend indicator, sometimes known as the Price_Volume period\n",
        "#also called Volume Price Trend\n",
        "def VPT(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        close_diff = (df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)\n",
        "        vpt = df['Volume'] * close_diff\n",
        "        vpt = vpt.cumsum()\n",
        "        data_frame[i]['VPT'] = vpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9f-83LWzjbr"
      },
      "outputs": [],
      "source": [
        "#Average Daily Trading Volume\n",
        "#typcially 20days or 30days\n",
        "def ADTV(data_frame, window_size = 30):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i][str(window_size) + 'day ADTV'] = df.Volume.rolling(window_size).sum()/window_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKUqtJF7zp5L"
      },
      "outputs": [],
      "source": [
        "#Volume Weighted Average Price\n",
        "def VWAP(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        cumulative_tp = df.pivot_point.cumsum()\n",
        "        cumulative_volume = df.Volume.cumsum()\n",
        "        data_frame[i]['VWAP'] = cumulative_tp/cumulative_volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7e7oYxTzp61"
      },
      "outputs": [],
      "source": [
        "#ICHIMOKU Cloud Trading\n",
        "#Tenkan_sen = Conversion Line, Kijun_sen = Base line, Senkou span= Leading Span \n",
        "#Ichimoku cloud is the area where located between Senkou span1 and span2\n",
        "def ichimoku_cloud(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        high_9 = df.High.rolling(window = 9).max()\n",
        "        low_9 = df.Low.rolling(window = 9).min()\n",
        "        high_26 = df.High.rolling(window = 26).max()\n",
        "        low_26 = df.Low.rolling(window = 26).min()\n",
        "        high_52 = df.High.rolling(window = 52).max()\n",
        "        low_52 = df.Low.rolling(window = 52).min()\n",
        "        data_frame[i]['Tenkan_sen'] = (high_9 + low_9)/2\n",
        "        data_frame[i]['Kijun_sen'] = (high_26 + low_26)/2\n",
        "        data_frame[i]['Senkou_span1'] = ((df.Tenkan_sen + df.Kijun_sen) / 2).shift(26)\n",
        "        data_frame[i]['Senkou_span2'] = ((high_52 + low_52) / 2).shift(26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClREMDgSzp8U"
      },
      "outputs": [],
      "source": [
        "#Disparity index\n",
        "#measures the relative position of an aseet's most recent closing price\n",
        "#value greater than zero- asset is gaining upward momentum, value less than zero- a sign that selling pressure is increasing\n",
        "def disparity(data_frame, window_size = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        mv = df.Close.rolling(window_size).mean()\n",
        "        data_frame[i]['Disparity'] = (df.Close - mv)/(mv * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5a3b6nzp9_"
      },
      "outputs": [],
      "source": [
        "#detrended price oscillaotor\n",
        "def DPO(data_frame, window_size = 20):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        detrended_period = int(window_size/2 + 1)\n",
        "        data_frame[i]['DPO'] = df['Close'].shift(detrended_period)  - df['Close'].rolling(window_size).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbRMnOSbzp_w"
      },
      "outputs": [],
      "source": [
        "#Ease of Movement\n",
        "def EMV(data_frame, window_size = 14):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        distance_moved = (df.High + df.Low)/2 - (df.High.shift(1) + df.Low.shift(1))/2\n",
        "        box_ratio = (df.Volume/1000000) / (df.High - df.Low)\n",
        "        emv = distance_moved/box_ratio\n",
        "        data_frame[i][str(window_size) + 'day EMV'] = emv.rolling(window_size).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8szn-Dk5zqCE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Force Index(FI)\n",
        "def ForceIndex(data_frame, window_size = 14):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        forceindex = (df.Close - df.Close.shift(1)) * df.Volume\n",
        "        data_frame[i]['1day_FI'] = forceindex\n",
        "        data_frame[i][str(window_size) + 'day FI'] = forceindex.ewm(span = window_size, adjust = False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ginyGfm8zqDZ"
      },
      "outputs": [],
      "source": [
        "#Keltner channel(KC)\n",
        "def Keltner_channel(data_frame, window_size = 14):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i]['KC_middle'] = df.Close.ewm(span = window_size, adjust = False).mean()\n",
        "        data_frame[i]['KC_upper'] = df.KC_middle + df.MF_Multiplier * df.ATR\n",
        "        data_frame[i]['KC_lower'] = df.KC_middle - df.MF_Multiplier * df.ATR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vkj0CWjzqFI"
      },
      "outputs": [],
      "source": [
        "#Know Sure Thing(KST)\n",
        "def Know_sure_thing(data_frame):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        ROC_10 = ((df.Close - df.Close.shift(10)) / df.Close.shift(10) * 100)\n",
        "        ROC_15 = ((df.Close - df.Close.shift(15)) / df.Close.shift(15) * 100)\n",
        "        ROC_20 = ((df.Close - df.Close.shift(20)) / df.Close.shift(20) * 100)\n",
        "        ROC_30 = ((df.Close - df.Close.shift(30)) / df.Close.shift(30) * 100)\n",
        "        RCMA1 = ROC_10.rolling(10).mean()\n",
        "        RCMA2 = ROC_15.rolling(10).mean()\n",
        "        RCMA3 = ROC_20.rolling(10).mean()\n",
        "        RCMA4 = ROC_30.rolling(15).mean()\n",
        "        data_frame[i]['KST'] = RCMA1 + RCMA2*2 + RCMA3*3 + RCMA4*4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVNq2PCqzjdb"
      },
      "outputs": [],
      "source": [
        "#mass index\n",
        "def Mass_index(data_frame, window_size = 9):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        difference = df.High - df.Low\n",
        "        ema9 = difference.ewm(span = window_size, adjust = False).mean()\n",
        "        ema9_of = ema9.ewm(span= window_size, adjust = False).mean()\n",
        "        mass = ema9 / ema9_of\n",
        "        data_frame[i]['MassIndex'] = mass.rolling(25).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAXqW7itzjfU"
      },
      "outputs": [],
      "source": [
        "#True Strength Index(TSI)\n",
        "def True_strength_index(data_frame, short_window=13, long_window=25):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "        close_diff = df.Close - df.Close.shift(1)\n",
        "        pcs = close_diff.ewm(span = long_window, adjust = False).mean()\n",
        "        pcds = pcs.ewm(span = short_window, adjust = False).mean()\n",
        "        abs_close_diff = abs(close_diff)\n",
        "        apcs = abs_close_diff.ewm(span = long_window, adjust = False).mean()\n",
        "        apcds = apcs.ewm(span = short_window, adjust = False).mean()\n",
        "        data_frame[i]['TSI'] = pcds/apcds * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA2hfU-Sz3xr"
      },
      "outputs": [],
      "source": [
        "#The average directional movement index\n",
        "#typicall 14 days\n",
        "#append Positive, Negative direction indicator and Average True range\n",
        "def ADX(data_frames, window_size = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frames):\n",
        "        high = df.High\n",
        "        low = df.Low\n",
        "        close = df.Close\n",
        "        true_range = pd.DataFrame(np.zeros((len(df), 3)), columns=['H-L', 'H-Cp', 'L-Cp'])\n",
        "        true_range['H-L'] = (high - low)\n",
        "        true_range['H-Cp'] = abs(high - close.shift(1))\n",
        "        true_range['L-Cp'] = abs(low - close.shift(1))\n",
        "        true_range = true_range.max(axis=1)\n",
        "        average_true_range = true_range.rolling(window_size).mean()\n",
        "        pos_direc_mv = (high - high.shift(1))\n",
        "        neg_direc_mv = (low - low.shift(1))\n",
        "        pos_direc_mv[pos_direc_mv < 0] = 0\n",
        "        neg_direc_mv[neg_direc_mv > 0] = 0\n",
        "        pos_direc_indi = 100 * (pos_direc_mv.ewm(alpha = 1/window_size).mean() / average_true_range)\n",
        "        neg_direc_indi = abs(100 * (neg_direc_mv.ewm(alpha = 1/window_size).mean() / average_true_range))\n",
        "        dx = (abs(pos_direc_indi - neg_direc_indi) / abs(pos_direc_indi + neg_direc_indi)) * 100\n",
        "        adx = ((dx.shift(1) * (window_size - 1)) + dx) / window_size\n",
        "        adx_smooth = adx.ewm(alpha = 1/window_size).mean()\n",
        "        data_frames[i]['TR'] = true_range\n",
        "        data_frames[i]['ATR'] = average_true_range\n",
        "        data_frames[i]['Pos_DI'] = pos_direc_indi\n",
        "        data_frames[i]['Neg_dI'] = neg_direc_indi\n",
        "        data_frames[i][str(window_size) + '-day ADX'] = adx_smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1CacjNTz33D"
      },
      "outputs": [],
      "source": [
        "#Ultimate Oscillator\n",
        "def ultimate_oscillator(data_frame):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        buying_pressure = df.Close - np.minimum(df.Low, df.Close.shift(1))\n",
        "        true_range = df.TR\n",
        "        average7 = buying_pressure.rolling(7).sum() / true_range.rolling(7).sum()\n",
        "        average14 = buying_pressure.rolling(14).sum() / true_range.rolling(14).sum()\n",
        "        average28 = buying_pressure.rolling(28).sum() / true_range.rolling(28).sum()\n",
        "        uo = (((average7*4) + (average14*2) + average28) / (4+2+1)) * 100\n",
        "        data_frame[i]['UO'] = uo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zku7xv8Nz344"
      },
      "outputs": [],
      "source": [
        "#Vortext indicator consists of plus and minus (VIN_pos and VIN_neg)\n",
        "#VM+ and VM- are uptrend and downtrend movement respectively \n",
        "def vortex_indicator(data_frame, window_size = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "        true_range = df.TR\n",
        "        data_frame[i]['VM_plus'] = abs(df.High - df.Low.shift(1))\n",
        "        data_frame[i]['VM_minus'] = abs(df.Low - df.High.shift(1))\n",
        "        sum_tr = true_range.rolling(window_size).sum()\n",
        "        sum_vm_plus = df.VM_plus.rolling(window_size).sum()\n",
        "        sum_vm_minus = df.VM_minus.rolling(window_size).sum()\n",
        "        data_frame[i]['VIN_pos'] = sum_vm_plus/sum_tr\n",
        "        data_frame[i]['VIN_neg'] = sum_vm_minus/sum_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5fSIieEz36p"
      },
      "outputs": [],
      "source": [
        "#Percentage price oscillator\n",
        "def ppo(data_frame, short_window = 12 , long_window = 26):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        short_ma = df.Close.ewm(span = short_window, adjust = False).mean()\n",
        "        long_ma = df.Close.ewm(span = long_window, adjust = False).mean()\n",
        "        ppo = 100 * (short_ma - long_ma) / long_ma\n",
        "        data_frame[i]['PPO'] = ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO7JZ_jcz38i"
      },
      "outputs": [],
      "source": [
        "#StochRSI \n",
        "#reading above 0.8 overbought , 0.2 oversold\n",
        "def stochrsi(data_frame):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        data_frame[i]['StochRSI'] = (df.RSI - df.RSI.rolling(14).min()) / (df.RSI.rolling(14).max() - df.RSI.rolling(14).min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSy5CiCqz8zA"
      },
      "outputs": [],
      "source": [
        "#Envelope(ENV)\n",
        "def Envelope(data_frame, window_size = 50):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        sma = df.Close.rolling(window_size).mean()\n",
        "        data_frame[i]['ENV_up_bound'] = sma + sma * 0.05\n",
        "        data_frame[i]['ENV_low_bound'] = sma - sma * 0.05\n",
        "        data_frame[i]['ENV_midpoint'] = sma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQgfmNLz3-S"
      },
      "outputs": [],
      "source": [
        "#Chande momentum oscillaotr\n",
        "def chande_momentum_oscillator(data_frame, window_size=14):\n",
        "\n",
        "    for i,df in enumerate(data_frame):\n",
        "        diff = df.Close.diff()\n",
        "        up, down = diff.copy(), diff.copy()\n",
        "        up[up < 0] = 0\n",
        "        down[down > 0] = 0\n",
        "        higher_sum = up.rolling(window_size).sum()\n",
        "        lower_sum = down.abs().rolling(window_size).sum()\n",
        "        data_frame[i]['cmo'] = ((higher_sum - lower_sum) / (higher_sum + lower_sum)) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Tz_-Zaz4AH"
      },
      "outputs": [],
      "source": [
        "#Kaufman Adaptive Moving Average\n",
        "def kama(data_frame, window_size=10, fast=2, slow=30):\n",
        "    for i, df in enumerate(data_frame):\n",
        "        volatility = abs(df['Close'] - df['Close'].shift(1)).rolling(window_size).sum()\n",
        "        change = abs(df['Close'] - df['Close'].shift(window_size))\n",
        "        er = change / volatility\n",
        "        sc = (er * (fast / (fast + 1) - slow / (slow + 1)) + slow / (slow + 1)) ** 2\n",
        "        kama = np.zeros(len(df))\n",
        "        isfirst = True\n",
        "        for j in range(len(df)):\n",
        "            if np.isnan(sc[j]):\n",
        "                kama[j] = np.nan\n",
        "            else:\n",
        "                if isfirst:\n",
        "                    kama[j] = df['Close'][j]\n",
        "                    isfirst = False\n",
        "                else:\n",
        "                    kama[j] = kama[j-1] + sc[j] * (df['Close'][j] - kama[j-1])\n",
        "        data_frame[i]['Kama'] = kama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pcHMwczjFm0"
      },
      "outputs": [],
      "source": [
        "def UlcerIndex(data_frame, window_size=14):\n",
        "    \"\"\"\n",
        "    Calculates the Ulcer Index for a given data frame.\n",
        "\n",
        "    Args:\n",
        "    data_frame: Pandas DataFrame containing OHLCV data.\n",
        "    window_size: The number of periods used to calculate the Ulcer Index.\n",
        "\n",
        "    Returns:\n",
        "    A Pandas DataFrame containing the Ulcer Index values.\n",
        "    \"\"\"\n",
        "    for i, df in enumerate(data_frame):\n",
        "      max_close = df['Close'].rolling(window=window_size).max()\n",
        "      percent_drawdown = (df['Close'] / max_close - 1) * 100\n",
        "      squared_drawdown = np.square(percent_drawdown)\n",
        "      ulcer_index = np.sqrt(squared_drawdown.rolling(window=window_size).mean())\n",
        "      data_frame[i]['Ulcer Index'] = ulcer_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuAYjdvMj5CF"
      },
      "outputs": [],
      "source": [
        "#Klinger_Oscillator\n",
        "def Klinger_Oscillator(data_frame, short_term=34, long_term=55, signal_period=13):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "      # Calculate the trend of money flow volume (MFV)\n",
        "      trend_MFV = df.MF_Volume.rolling(window=short_term).sum() - df.MF_Volume.rolling(window=long_term).sum()\n",
        "      \n",
        "      # Calculate the trend of the volume force (VF)\n",
        "      volume_force = df['Volume'] * ((2 * df['Close']) - df['High'] - df['Low']) / (df['High'] - df['Low'])\n",
        "      trend_VF = volume_force.rolling(window=short_term).sum() - volume_force.rolling(window=long_term).sum()\n",
        "      # Calculate the oscillator\n",
        "      oscillator = trend_MFV - trend_VF\n",
        "      # Calculate the signal line\n",
        "      signal = oscillator.rolling(window=signal_period).mean()\n",
        "      data_frame[i]['Klinger_Oscillator'] = oscillator\n",
        "      \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI7n27iklac9"
      },
      "outputs": [],
      "source": [
        "#Mcclellan_Osciilator\n",
        "def McClellan_Oscillator(data_frame, short_period=19, long_period=39):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "      ema_short = df.Close.ewm(span=short_period, adjust=False).mean()\n",
        "      ema_long = df.Close.ewm(span=long_period, adjust=False).mean()\n",
        "\n",
        "      oscillator = ema_short - ema_long\n",
        "\n",
        "      ema_signal = oscillator.ewm(span=9, adjust=False).mean()\n",
        "      data_frame[i]['McClellan_Osci'] = oscillator - ema_signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbgXULu2qUvC"
      },
      "outputs": [],
      "source": [
        "def choppiness_index(data_frame, window_size = 14):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "      high = df['High']\n",
        "      low = df['Low']\n",
        "      close = df['Close']\n",
        "\n",
        "      atr = df.ATR\n",
        "      tr = high - low\n",
        "\n",
        "      # Calculate the True Range High (TRH) and True Range Low (TRL) for the lookback period\n",
        "      trh = high.rolling(window=window_size).max()\n",
        "      trl = low.rolling(window=window_size).min()\n",
        "\n",
        "      # Calculate the absolute difference between the current close and the TRH\n",
        "      diff_high = abs(close - trh)\n",
        "\n",
        "      # Calculate the absolute difference between the current close and the TRL\n",
        "      diff_low = abs(close - trl)\n",
        "\n",
        "      # Calculate the Sum of TR over the lookback period\n",
        "      sum_tr = tr.rolling(window=window_size).sum()\n",
        "\n",
        "      # Calculate the Sum of the Absolute Differences over the lookback period\n",
        "      sum_diff = diff_high.rolling(window=window_size).sum() + diff_low.rolling(window=window_size).sum()\n",
        "\n",
        "      # Calculate the Raw Choppiness Index\n",
        "      data_frame[i]['raw_ci'] = 100 * np.log10(sum_tr / sum_diff)\n",
        "\n",
        "      # Smooth the Raw Choppiness Index with a 3 period EMA\n",
        "      data_frame[i]['Ci'] = df.raw_ci.ewm(span = 3, adjust = False).mean()\n",
        "\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx_XjPAfsVu8"
      },
      "outputs": [],
      "source": [
        "#elder_ray_index\n",
        "def elder_ray_index(data_frame, period = 13):\n",
        "    \n",
        "    for i, df in enumerate(data_frame):\n",
        "      ema = df['Close'].ewm(span=period, adjust=False).mean()\n",
        "      data_frame[i]['bull_power'] = df['High'] - ema\n",
        "      data_frame[i]['bear_power'] = df['Low'] - ema\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E25HEBNIs8GB"
      },
      "outputs": [],
      "source": [
        "def SEB(data_frame, window_size=20, num_std=2):\n",
        "\n",
        "    for i, df in enumerate(data_frame):\n",
        "      se = df.Close.rolling(window_size).std() / np.sqrt(window_size)\n",
        "      data_frame[i]['SEB_upper'] = df.Close.rolling(window_size).mean() + num_std * se\n",
        "      data_frame[i]['SEB_lower'] = df.Close.rolling(window_size).mean() - num_std * se\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJi8zb-TvXD"
      },
      "source": [
        "#Task3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKkoX3pS0GN4"
      },
      "source": [
        "#Main Execution Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZYlqonhsp29"
      },
      "source": [
        "#Task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AuvCy5i0BVF",
        "outputId": "33df3cba-cb46-44e4-f385-16b1c4739194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 Threshold: -0.002114146149282713\n",
            "Level 2 Threshold: 0.0019722866658953677\n",
            "Number of data points for each level: [1465866 1465866 1470268]\n",
            "Threshold 1: -0.21141461492827127\n",
            "Threshold 2: 0.19722866658953678\n",
            "Number of increases: 1470268\n",
            "Number of decreases: 1465866\n",
            "Number of no big changes: 1465866\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def daily_percentage_changes(stock_data):\n",
        "    close_prices = stock_data['Close']\n",
        "    shifted_close_prices = close_prices.shift(-1)[:-1] # Shift the close prices by -1 (upwards)\n",
        "    today_close_prices = close_prices[:-1] # Remove the last value from close_prices\n",
        "    percentage_changes = (shifted_close_prices - today_close_prices) / today_close_prices\n",
        "    stock_data['Close_pct_change'] = percentage_changes\n",
        "    return percentage_changes\n",
        "\n",
        "\n",
        "percentage_changes = [daily_percentage_changes(stock) for stock in data_frame]\n",
        "flattened_changes = sorted(np.concatenate(percentage_changes))\n",
        "\n",
        "total_data_points = len(flattened_changes)\n",
        "level_1_threshold = np.nanpercentile(flattened_changes, 33.3)\n",
        "level_2_threshold = np.nanpercentile(flattened_changes, 66.6)\n",
        "\n",
        "\n",
        "def assign_level(percentage_change, level_1_threshold, level_2_threshold):\n",
        "    if percentage_change < level_1_threshold:\n",
        "        return 0  # Decrease\n",
        "    elif percentage_change < level_2_threshold:\n",
        "        return 1  # No big change\n",
        "    else:\n",
        "        return 2  # Increase\n",
        "\n",
        "assigned_levels = [assign_level(change, level_1_threshold, level_2_threshold) for change in flattened_changes]\n",
        "level_counts = np.bincount(assigned_levels)\n",
        "\n",
        "print(\"Level 1 Threshold:\", level_1_threshold)\n",
        "print(\"Level 2 Threshold:\", level_2_threshold)\n",
        "print(\"Number of data points for each level:\", level_counts)\n",
        "\n",
        "#Task1\n",
        "closed_percentage = daily_percentage_change_for_close(data)\n",
        "thresholds = find_thresholds(closed_percentage)\n",
        "levels = divide_into_levels(closed_percentage, thresholds)\n",
        "threshold_number_of_data_points(levels, thresholds)\n",
        "labels = [0, 1, 2]\n",
        "label_vectors = create_label_vectors(levels, labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjwvMsEtoxxq"
      },
      "outputs": [],
      "source": [
        "#print(data_frame[0].iloc[1:101])\n",
        "#print(label_vectors[0][101:102])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG8Ni2X8hF2p",
        "outputId": "f2c21183-d475-4d1c-a7f7-b1b926a6657b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Open      High       Low     Close    Volume  Close_pct_change\n",
            "2197  0.680849  0.683633  0.672997  0.674055  0.351048          0.000000\n",
            "2198  0.674389  0.674890  0.671326  0.674055  0.143069          0.002892\n",
            "2199  0.674110  0.676227  0.672440  0.676004  0.312048         -0.009885\n",
            "2200  0.675725  0.675837  0.668820  0.669321  0.279230         -0.007655\n",
            "2201  0.669293  0.671326  0.662973  0.664198  0.552933               NaN\n"
          ]
        }
      ],
      "source": [
        "print(data_frame[0].tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HooVmM4rstZP"
      },
      "source": [
        "#Task2 functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFRJvTdO0dlG"
      },
      "outputs": [],
      "source": [
        "#Taks2\n",
        "#Indicators\n",
        "daily_return(data_frame)\n",
        "daily_log_return(data_frame)\n",
        "standard_deviation(data_frame,5)\n",
        "standard_deviation(data_frame,20)\n",
        "median_price(data_frame)\n",
        "volatility(data_frame)\n",
        "ROC(data_frame, 5)\n",
        "ROC(data_frame, 10)\n",
        "ROC(data_frame, 14)\n",
        "ROC(data_frame, 20)\n",
        "RSI(data_frame)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk3PFyYk0i1u"
      },
      "outputs": [],
      "source": [
        "#Moving average 20, 50 ,and 200 periods\n",
        "moving_average(data_frame, 5)\n",
        "moving_average(data_frame, 10)\n",
        "moving_average(data_frame, 20)\n",
        "moving_average(data_frame, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHE4lF430jAl"
      },
      "outputs": [],
      "source": [
        "#short term Exponential Moving average(EMA)\n",
        "ema(data_frame, 12)\n",
        "ema(data_frame, 26)\n",
        "#long term EMA\n",
        "ema(data_frame, 50)\n",
        "ema(data_frame, 100) #chagne the period\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtG1kY7JI05y"
      },
      "outputs": [],
      "source": [
        "WMA(data_frame)\n",
        "bollinger_bands(data_frame, 20)\n",
        "bollinger_bands(data_frame, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWNedUy_Ly1A"
      },
      "outputs": [],
      "source": [
        "momentum(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaTqdd--ezH8"
      },
      "outputs": [],
      "source": [
        "True_strength_index(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg_-ve7Ze82A"
      },
      "outputs": [],
      "source": [
        "ADX(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UlXpJIlfOEc"
      },
      "outputs": [],
      "source": [
        "MACD(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrKIZiVOfyrM"
      },
      "outputs": [],
      "source": [
        "awesome_Oscillator(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_F_nD30qCS"
      },
      "outputs": [],
      "source": [
        "stochastic_oscillator(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pssRNeVtgAPU"
      },
      "outputs": [],
      "source": [
        "williams_R(data_frame)\n",
        "CCI(data_frame)\n",
        "#coppock_curve(data_frame) may be not appropriate for this project\n",
        "on_balance_volume(data_frame)\n",
        "ADTV(data_frame)\n",
        "Fibonacci_retracement_levels(data_frame)\n",
        "money_flow_index(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85pWFy_DhIY3"
      },
      "outputs": [],
      "source": [
        "TEMA(data_frame)\n",
        "VPT(data_frame)\n",
        "pivot_points(data_frame)\n",
        "VWAP(data_frame)\n",
        "ichimoku_cloud(data_frame)\n",
        "disparity(data_frame)\n",
        "DPO(data_frame)\n",
        "donchian_channel(data_frame)\n",
        "EMV(data_frame)\n",
        "vortex_indicator(data_frame)\n",
        "ultimate_oscillator(data_frame)\n",
        "chaikin_money_flow(data_frame)\n",
        "accumulation_distribution(data_frame)\n",
        "ForceIndex(data_frame)\n",
        "Keltner_channel(data_frame)\n",
        "Know_sure_thing(data_frame)\n",
        "Mass_index(data_frame)\n",
        "ppo(data_frame) \n",
        "stochrsi(data_frame)\n",
        "Envelope(data_frame)\n",
        "chande_momentum_oscillator(data_frame)\n",
        "kama(data_frame)\n",
        "Volume_Oscillator(data_frame)\n",
        "zero_lag_ema(data_frame)\n",
        "Hull_MA(data_frame)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_xCtO_hY0lp"
      },
      "outputs": [],
      "source": [
        "#aroon_indicator(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZN6MpvxjEgJ"
      },
      "outputs": [],
      "source": [
        "UlcerIndex(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAt7klbOkibN"
      },
      "outputs": [],
      "source": [
        "Klinger_Oscillator(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeTiSSg_lzHy"
      },
      "outputs": [],
      "source": [
        "McClellan_Oscillator(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrJVvNJhrkGA"
      },
      "outputs": [],
      "source": [
        "choppiness_index(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJyZvWqJsu7P"
      },
      "outputs": [],
      "source": [
        "elder_ray_index(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3Cv8l9QtHlT"
      },
      "outputs": [],
      "source": [
        "SEB(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2EXrx9U01gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612ca580-4559-487a-c5cb-90c97194af6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n",
            "          Open      High       Low     Close    Volume  Close_pct_change  \\\n",
            "2197  0.680849  0.683633  0.672997  0.674055  0.351048          0.000000   \n",
            "2198  0.674389  0.674890  0.671326  0.674055  0.143069          0.002892   \n",
            "2199  0.674110  0.676227  0.672440  0.676004  0.312048         -0.009885   \n",
            "2200  0.675725  0.675837  0.668820  0.669321  0.279230         -0.007655   \n",
            "2201  0.669293  0.671326  0.662973  0.664198  0.552933          0.000000   \n",
            "\n",
            "      Daily_return  log_return  5-day S.D  20-day S.D    median  volatility  \\\n",
            "2197     -0.006794   -0.010192   0.003615    0.008223  0.678315    0.039817   \n",
            "2198     -0.000334    0.000000   0.003862    0.008374  0.673108    0.035028   \n",
            "2199      0.001893    0.002887   0.003624    0.008290  0.674333    0.036086   \n",
            "2200     -0.006404   -0.009935   0.004198    0.007898  0.672328    0.040820   \n",
            "2201     -0.005096   -0.007684   0.004780    0.007668  0.667149    0.033664   \n",
            "\n",
            "      5-day ROC  10-day ROC  14-day ROC  20-day ROC        RSI  5-day MA  \\\n",
            "2197   0.190384    2.802788    1.611818    3.003999  60.584343  0.677262   \n",
            "2198  -0.247235    2.057338    1.433000    1.705738  59.650115  0.676928   \n",
            "2199   0.264310    2.490710    1.675182    2.933943  60.928969  0.677285   \n",
            "2200  -1.765424    1.178552    1.468979    1.907750  59.245524  0.674879   \n",
            "2201  -2.461565   -1.425684    1.299472    0.777354  57.952181  0.671526   \n",
            "\n",
            "      10-day MA  20-day MA  50-day MA  12-day EMA  26-day EMA  50-day EMA  \\\n",
            "2197   0.671446   0.666015   0.679632    0.671869    0.671121    0.678203   \n",
            "2198   0.672804   0.666580   0.678695    0.672205    0.671339    0.678040   \n",
            "2199   0.674447   0.667543   0.677725    0.672790    0.671684    0.677960   \n",
            "2200   0.675227   0.668170   0.676681    0.672256    0.671509    0.677622   \n",
            "2201   0.674266   0.668426   0.675548    0.671016    0.670967    0.677095   \n",
            "\n",
            "      100-day EMA  14-day WMA  20-day BOLU  20-day BOLD  50-day BOLU  \\\n",
            "2197     0.693865    0.672103     0.682460     0.649569     0.722108   \n",
            "2198     0.693473    0.672843     0.683327     0.649832     0.719484   \n",
            "2199     0.693127    0.673751     0.684122     0.650964     0.716315   \n",
            "2200     0.692655    0.673662     0.683967     0.652373     0.713206   \n",
            "2201     0.692092    0.672798     0.683761     0.653091     0.709935   \n",
            "\n",
            "      50-day BOLD  14-day Momentum       TSI        TR       ATR     Pos_DI  \\\n",
            "2197     0.637157         0.010692  2.516577  0.010637  0.007802  36.861058   \n",
            "2198     0.637905         0.009523  3.115648  0.003564  0.007667  34.831909   \n",
            "2199     0.639134         0.011138  4.178457  0.003787  0.007522  34.237451   \n",
            "2200     0.640156         0.009690  2.887699  0.007184  0.007418  32.235153   \n",
            "2201     0.641162         0.008520  0.264424  0.008353  0.007601  29.212100   \n",
            "\n",
            "         Neg_dI  14-day ADX      MACD  signal Line        AO  14-day Stoch  \\\n",
            "2197  20.384377   24.852471  0.000748    -0.002178  0.010125     55.188708   \n",
            "2198  20.818707   25.114813  0.000867    -0.001569  0.010081     55.188708   \n",
            "2199  19.704809   25.128499  0.001105    -0.001034  0.010417     60.691828   \n",
            "2200  22.037831   25.216379  0.000747    -0.000678  0.008884     41.823953   \n",
            "2201  25.465807   24.696371  0.000049    -0.000533  0.006200     22.483163   \n",
            "\n",
            "      3_day Stoch  14-day William %R  20-day CCI        OBV   OBV_Mean  \\\n",
            "2197    68.553462         -44.811292   90.622613  25.140472  25.463579   \n",
            "2198    61.687647         -44.811292   57.739841  25.140472  25.470090   \n",
            "2199    57.023081         -39.308172   66.015813  25.452520  25.479357   \n",
            "2200    52.568163         -58.176047   28.428655  25.173290  25.485499   \n",
            "2201    41.666315         -77.516837  -24.733717  24.620358  25.492746   \n",
            "\n",
            "      30day ADTV  FRL_lev1  FRL_lev2  FRL_lev3        MF        MFI      TEMA  \\\n",
            "2197    0.393091  0.911502  0.857581   0.77042  0.237622  44.669275  0.677018   \n",
            "2198    0.381933  0.911502  0.857581   0.77042  0.096346  40.336495  0.677327   \n",
            "2199    0.383105  0.911502  0.857581   0.77042  0.210598  39.333264  0.678182   \n",
            "2200    0.383450  0.911502  0.857581   0.77042  0.187454  39.044411  0.676506   \n",
            "2201    0.393122  0.911502  0.857581   0.77042  0.368345  38.727572  0.673349   \n",
            "\n",
            "           VPT  pivot_point  Resistance1  Resistance2  Support1  Supprot2  \\\n",
            "2197  0.013681     0.676895     0.680793     0.687531  0.670156  0.666258   \n",
            "2198  0.013681     0.673424     0.675521     0.676988  0.671957  0.669859   \n",
            "2199  0.014583     0.674890     0.677340     0.678677  0.673553  0.671103   \n",
            "2200  0.011823     0.671326     0.673832     0.678343  0.666815  0.664309   \n",
            "2201  0.007590     0.666165     0.669358     0.674519  0.661005  0.657812   \n",
            "\n",
            "          VWAP  Tenkan_sen  Kijun_sen  Senkou_span1  Senkou_span2  Disparity  \\\n",
            "2197  3.013052    0.673331   0.671409      0.683299      0.694047   0.000083   \n",
            "2198  3.013468    0.673331   0.671409      0.682909      0.694047   0.000073   \n",
            "2199  3.013012    0.675725   0.671409      0.682505      0.694047   0.000090   \n",
            "2200  3.012721    0.679373   0.671409      0.682505      0.694047  -0.000020   \n",
            "2201  3.011009    0.676449   0.671409      0.679735      0.692850  -0.000106   \n",
            "\n",
            "           DPO  Higher_Channel  Lower_Channel  Middle_Channel  14day EMV  \\\n",
            "2197 -0.006383        0.689926       0.654174        0.672050  33.745346   \n",
            "2198 -0.010902        0.689926       0.654508        0.672217  20.183166   \n",
            "2199 -0.007077        0.689926       0.654508        0.672217  17.160800   \n",
            "2200 -0.008594        0.689926       0.654508        0.672217  25.044338   \n",
            "2201 -0.006901        0.689926       0.654508        0.672217  22.961732   \n",
            "\n",
            "       VM_plus  VM_minus   VIN_pos   VIN_neg         UO  MF_Multiplier  \\\n",
            "2197  0.005402  0.008966  1.101457  0.788928  50.986271      -0.801045   \n",
            "2198  0.001893  0.012307  1.058628  0.887414  46.029491       0.531245   \n",
            "2199  0.004901  0.002450  1.046538  0.899784  50.713852       0.882350   \n",
            "2200  0.003397  0.007407  1.066491  0.871845  45.386593      -0.857137   \n",
            "2201  0.002506  0.012864  1.041865  0.858707  41.414338      -0.706675   \n",
            "\n",
            "      MF_Volume       CMF       ADI   1day_FI  14day FI  KC_middle  KC_upper  \\\n",
            "2197  -0.281205 -0.199189  3.242357 -0.002424  0.000348   0.671195  0.664945   \n",
            "2198   0.076004 -0.184016  3.318361  0.000000  0.000302   0.671576  0.675649   \n",
            "2199   0.275336 -0.205979  3.593697  0.000608  0.000343   0.672166  0.678803   \n",
            "2200  -0.239338 -0.197278  3.354359 -0.001866  0.000048   0.671787  0.665428   \n",
            "2201  -0.390744 -0.185212  2.963615 -0.002833 -0.000336   0.670775  0.665403   \n",
            "\n",
            "      KC_lower       KST  MassIndex       PPO  StochRSI  ENV_up_bound  \\\n",
            "2197  0.677444 -6.681927  24.963555  0.111395  0.700616      0.713614   \n",
            "2198  0.667503 -2.513335  25.033497  0.129084  0.678871      0.712629   \n",
            "2199  0.665530  1.603975  24.928458  0.164575  0.708638      0.711611   \n",
            "2200  0.678145  4.137352  24.780671  0.111221  0.669454      0.710515   \n",
            "2201  0.676147  5.035934  24.771631  0.007262  0.475450      0.709326   \n",
            "\n",
            "      ENV_low_bound  ENV_midpoint        cmo      Kama        VO     ZLEMA  \\\n",
            "2197       0.645651      0.679632  21.168686  0.676035  6.057598  0.677037   \n",
            "2198       0.644760      0.678695  19.300231  0.674601  5.458169  0.673741   \n",
            "2199       0.643838      0.677725  21.857938  0.675569  5.688082  0.675145   \n",
            "2200       0.642847      0.676681  18.491048  0.670392  6.183538  0.671626   \n",
            "2201       0.641771      0.675548  15.904361  0.665575  5.347109  0.665214   \n",
            "\n",
            "           HMA  Ulcer Index  Klinger_Oscillator  McClellan_Osci     raw_ci  \\\n",
            "2197  0.667434     1.131414       -4.440892e-16        0.002612 -52.094214   \n",
            "2198  0.667973     1.062031       -6.661338e-16        0.002395 -54.236404   \n",
            "2199  0.668619     0.974888       -4.440892e-16        0.002269 -56.411697   \n",
            "2200  0.668806     0.845131       -6.661338e-16        0.001863 -58.383797   \n",
            "2201  0.668656     0.991464       -2.220446e-16        0.001339 -58.479502   \n",
            "\n",
            "             Ci  bull_power  bear_power  SEB_upper  SEB_lower  \n",
            "2197 -51.506668    0.012133    0.001496   0.669692   0.662337  \n",
            "2198 -52.871536    0.003025   -0.000539   0.670325   0.662835  \n",
            "2199 -54.641616    0.003770   -0.000017   0.671250   0.663836  \n",
            "2200 -56.512707    0.003828   -0.003189   0.671702   0.664637  \n",
            "2201 -57.496104    0.000433   -0.007920   0.671855   0.664997  \n"
          ]
        }
      ],
      "source": [
        "#replace all the NaN to 0\n",
        "for i in range(len(data_frame)):\n",
        "    data_frame[i].fillna(0, inplace = True)\n",
        "\n",
        "#display all the columns\n",
        "pd.set_option('display.max_columns', None) \n",
        "print(len(data_frame[0].columns))\n",
        "print(data_frame[0].tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Lx9wzHE9zf"
      },
      "source": [
        "#Train dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65pEEI_Ltt2Q"
      },
      "source": [
        "#Task3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_features(stock_data):\n",
        "    # Replace infinite values with NaN\n",
        "    stock_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    \n",
        "    # Handle NaN values (either fill them with appropriate values or drop rows/columns)\n",
        "    # Option 1: Fill NaN values with the mean of the respective columns\n",
        "    stock_data.fillna(0, inplace=True)\n",
        "    \n",
        "    # Option 2: Drop rows containing NaN values (uncomment the line below if you want to use this option)\n",
        "    # stock_data.dropna(inplace=True)\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_data = scaler.fit_transform(stock_data)\n",
        "    return pd.DataFrame(normalized_data, columns=stock_data.columns)\n",
        "\n",
        "\n",
        "normalized_stock_data = [normalize_features(stock) for stock in data_frame]"
      ],
      "metadata": {
        "id": "b49rBykpX6CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(training_data, train_ratio=0.8):\n",
        "    split_index = int(len(training_data) * train_ratio)\n",
        "    train_data = training_data[:split_index]\n",
        "    val_data = training_data[split_index:]\n",
        "    return train_data, val_data\n",
        "\n",
        "train_data_X, validation_data_X = split_data(normalized_stock_data) #oringal data frame with features added so 2000*2202*102\n",
        "train_data_Y, validation_data_Y = split_data(label_vectors) #3d vectors consisted of 3 features(decrease, no big change, increase) 2000*2201*3\n",
        "\n",
        "train_data_X = np.array(train_data_X)\n",
        "validation_data_X = np.array(validation_data_X)\n",
        "train_data_Y = np.array(train_data_Y)\n",
        "validation_data_Y = np.array(validation_data_Y)\n",
        "\n",
        "#add (0,0,0) to match day since it has 2202\n",
        "padded_train_data_Y = np.pad(train_data_Y, ((0, 0), (0, 1), (0, 0)), mode='constant', constant_values=0)\n",
        "padded_validation_data_Y = np.pad(validation_data_Y, ((0, 0), (0, 1), (0, 0)), mode='constant', constant_values=0)"
      ],
      "metadata": {
        "id": "ARgVuK8mYnnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBmvkfU4GljN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a968af-295d-4bd8-bdaa-bfd7956ac6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data_X shape: (1600, 2202, 105)\n",
            "validation_data_X shape: (400, 2202, 105)\n",
            "padded_train_data_Y shape: (1600, 2202, 3)\n",
            "padded_validation_data_Y shape: (400, 2202, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"train_data_X shape:\", train_data_X.shape)\n",
        "print(\"validation_data_X shape:\", validation_data_X.shape)\n",
        "print(\"padded_train_data_Y shape:\", padded_train_data_Y.shape)\n",
        "print(\"padded_validation_data_Y shape:\", padded_validation_data_Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"feature selection for task4\"\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def select_features(data_X, data_Y, k=75):\n",
        "    num_features = data_X.shape[2]\n",
        "    correlations = np.zeros(num_features)\n",
        "\n",
        "    target = data_Y[:, :, 2].flatten()\n",
        "\n",
        "    for i in range(num_features):\n",
        "        feature = data_X[:, :, i].flatten()\n",
        "\n",
        "        if np.std(feature) == 0 or np.std(target) == 0:\n",
        "            continue\n",
        "\n",
        "        corr, _ = pearsonr(feature, target)\n",
        "        correlations[i] = abs(corr)\n",
        "\n",
        "    top_k_indices = np.argsort(correlations)[-k:][::-1]\n",
        "    selected_data_X = data_X[:, :, top_k_indices]\n",
        "    \n",
        "    # Return top feature indices and their correlations as well\n",
        "    top_k_correlations = correlations[top_k_indices]\n",
        "    \n",
        "    return selected_data_X, top_k_indices, top_k_correlations\n"
      ],
      "metadata": {
        "id": "Ha0f6JHYRJat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w9RP7r1Ie3Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "class StockDataset(data.Dataset):\n",
        "    def __init__(self, data_X, data_Y, window_size=100):\n",
        "        self.data_X = data_X\n",
        "        self.data_Y = data_Y\n",
        "        self.window_size = window_size\n",
        "        self.num_stocks, self.num_days, self.num_features = data_X.shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_stocks * (self.num_days - self.window_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stock_idx = idx // (self.num_days - self.window_size)\n",
        "        day_idx = idx % (self.num_days - self.window_size)\n",
        "\n",
        "        features = self.data_X[stock_idx, day_idx : day_idx + self.window_size, :]\n",
        "        label = self.data_Y[stock_idx, day_idx + self.window_size, :]\n",
        "\n",
        "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "window_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlh5Wltdv04Q"
      },
      "outputs": [],
      "source": [
        "#features, label = train_dataset[0]\n",
        "#print(normalized_stock_data[0].iloc[0:100]) #print 0~99 trading records 100\n",
        "#print(label_vectors[0][100:101]) #print 101st label\n",
        "\n",
        "#print(len(train_dataset))\n",
        "#print(features.shape)\n",
        "#print(label.shape)\n",
        "\n",
        "#print(len(features))\n",
        "#print(label)\n",
        "\n",
        "#print(len(train_dataset))\n",
        "\n",
        "#2000*2102*0.8\n",
        "\n",
        "#print(len(train_dataset))\n",
        "#print(len(validation_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKHY3_TuKph6"
      },
      "outputs": [],
      "source": [
        "#LSTM uses 2d vector\n",
        "class StockPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(StockPredictionModel, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # LSTM layer\n",
        "        out, _ = self.lstm(x)\n",
        "        \n",
        "        # Get the output from the last time step of the LSTM layer\n",
        "        out = out[:, -1, :]\n",
        "        \n",
        "        # Fully connected layers\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = train_data_X.shape[2] #after feature selection. If we do no apply feature selection, it should be just train_x, right now we didn't apply feature selection\n",
        "hidden_size = 512\n",
        "num_layers = 3\n",
        "num_classes = train_data_Y.shape[2]\n",
        "\n",
        "model = StockPredictionModel(input_size, hidden_size, num_layers, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loaders = []\n",
        "stock_batch_size = 160\n",
        "num_days = 500\n",
        "\n",
        "for i in range(0, len(train_data_X), stock_batch_size): #load 160 stock\n",
        "    temp_train_data = []\n",
        "    temp_label_data = []\n",
        "    for stock, label in zip(train_data_X[i:i+stock_batch_size], padded_train_data_Y[i:i+stock_batch_size]):\n",
        "        temp_train_data.append(stock[:num_days])\n",
        "        temp_label_data.append(label[:num_days])\n",
        "    \n",
        "    # Convert lists to numpy arrays\n",
        "    temp_train_data = np.array(temp_train_data)\n",
        "    temp_label_data = np.array(temp_label_data)\n",
        "    \n",
        "    train_dataset = StockDataset(temp_train_data, temp_label_data, window_size=100)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "    train_loaders.append(train_dataloader)\n",
        "\n",
        "validation_loaders = []\n",
        "for i in range(0, len(validation_data_X), 40): #load 40 stocks\n",
        "    validation_dataset = StockDataset(validation_data_X[i:i+40], padded_validation_data_Y[i:i+40], window_size=100)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=1024, shuffle=True)\n",
        "    validation_loaders.append(validation_dataloader)\n"
      ],
      "metadata": {
        "id": "ei1risqEaCp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    true_positive = 0\n",
        "    positive_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, true_labels = torch.max(labels.data, 1)\n",
        "            \n",
        "            total += true_labels.size(0)\n",
        "            correct += (predicted == true_labels).sum().item()\n",
        "            positive_predictions += (predicted == 2).sum().item()\n",
        "            true_positive += ((predicted == 2) & (true_labels == 2)).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    if positive_predictions > 0:\n",
        "      precision = true_positive / positive_predictions\n",
        "    else:\n",
        "      precision = 0\n",
        "    positive_prediction_percentage = positive_predictions / total\n",
        "\n",
        "    return accuracy, precision, positive_prediction_percentage\n",
        "\n",
        "best_accuracy = 0.0 # initialize the best accuracy to 0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Starting epoch {epoch + 1} of {num_epochs}\")\n",
        "    mini_epoch = 0\n",
        "    for loaders in zip(train_loaders, validation_loaders):\n",
        "        train_dataloader, validation_dataloader = loaders\n",
        "        model.train() # set the model to training mode\n",
        "\n",
        "\n",
        "        for i, (features, labels) in enumerate(train_dataloader):\n",
        "            #print(f\"Number of batches {i + 1}: {len(train_dataloader)}\")\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, torch.max(labels, 1)[1].long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        mini_epoch += 1\n",
        "        # Evaluate the model on training data\n",
        "        print(f\"mini_epoch [{mini_epoch}/{10}]\")\n",
        "        train_accuracy, train_precision, train_positive_prediction_percentage = evaluate_model(model, train_dataloader)\n",
        "        print(f\"Training Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Positive Predictions: {train_positive_prediction_percentage:.4f}\")\n",
        "\n",
        "        # Evaluate the model on validation data\n",
        "        validation_accuracy, validation_precision, validation_positive_prediction_percentage = evaluate_model(model, validation_dataloader)\n",
        "        print(f\"Validation Accuracy: {validation_accuracy:.4f}, Precision: {validation_precision:.4f}, Positive Predictions: {validation_positive_prediction_percentage:.4f}\")\n",
        "\n",
        "        # Check if the validation accuracy has improved\n",
        "        if validation_accuracy > best_accuracy:\n",
        "            best_accuracy = validation_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth') # save the model weights\n",
        "\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "#The model is trained for num_epochs epochs, with each epoch consisting of 40 mini-epochs. In each mini-epoch, the model is trained on a batch of 1024 samples, \n",
        "# and the training and validation accuracies, precisions, and percentages of positive predictions are printed.\n",
        "# A training epoch can be divided into multiple mini-epochs, each consisting of multiple batches of training data. The term \"mini-epoch\" is not a standard term in machine learning,\n",
        "# but it is a useful way to divide the training process into smaller parts, especially when dealing with large datasets or models that take a long time to train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xy2KxPu_UjA",
        "outputId": "79605e15-da13-4ef8-da5d-c8d80f13d4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.3976, Precision: 0.3031, Positive Predictions: 0.0435\n",
            "Validation Accuracy: 0.3683, Precision: 0.2449, Positive Predictions: 0.0445\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4194, Precision: 0.3459, Positive Predictions: 0.2041\n",
            "Validation Accuracy: 0.3960, Precision: 0.2923, Positive Predictions: 0.2129\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4429, Precision: 0.3694, Positive Predictions: 0.3661\n",
            "Validation Accuracy: 0.4143, Precision: 0.3518, Positive Predictions: 0.3157\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4135, Precision: 0.3719, Positive Predictions: 0.3543\n",
            "Validation Accuracy: 0.4010, Precision: 0.3424, Positive Predictions: 0.2842\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4693, Precision: 0.3853, Positive Predictions: 0.2852\n",
            "Validation Accuracy: 0.3956, Precision: 0.3773, Positive Predictions: 0.3901\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4865, Precision: 0.3860, Positive Predictions: 0.3603\n",
            "Validation Accuracy: 0.4120, Precision: 0.3660, Positive Predictions: 0.4407\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4515, Precision: 0.3731, Positive Predictions: 0.1652\n",
            "Validation Accuracy: 0.4726, Precision: 0.3527, Positive Predictions: 0.1230\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4748, Precision: 0.3797, Positive Predictions: 0.3431\n",
            "Validation Accuracy: 0.4631, Precision: 0.3612, Positive Predictions: 0.3130\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4446, Precision: 0.3918, Positive Predictions: 0.4221\n",
            "Validation Accuracy: 0.4303, Precision: 0.3751, Positive Predictions: 0.4615\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4349, Precision: 0.3933, Positive Predictions: 0.3699\n",
            "Validation Accuracy: 0.4344, Precision: 0.3695, Positive Predictions: 0.3753\n",
            "Best validation accuracy: 0.4726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task5"
      ],
      "metadata": {
        "id": "SIkziEmyXKV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_models = 10\n",
        "models = [StockPredictionModel(input_size, hidden_size, num_layers, num_classes) for _ in range(num_models)]\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training model {i + 1} of {num_models}\")\n",
        "    model.to(device)\n",
        "    \n",
        "    # Train the model\n",
        "    num_epochs = 10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Starting epoch {epoch + 1} of {num_epochs}\")\n",
        "        mini_epoch = 0\n",
        "        \n",
        "        for loaders in zip(train_loaders, validation_loaders):\n",
        "            train_dataloader, validation_dataloader = loaders\n",
        "            model.train()  # set the model to training mode\n",
        "\n",
        "            for _, (features, labels) in enumerate(train_dataloader):\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, torch.max(labels, 1)[1].long())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            mini_epoch += 1\n",
        "            print(f\"mini_epoch [{mini_epoch}/{10}]\")\n",
        "\n",
        "            # Evaluate the model on training data\n",
        "            train_accuracy, train_precision, train_positive_prediction_percentage = evaluate_model(model, train_dataloader)\n",
        "            print(f\"Training Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Positive Predictions: {train_positive_prediction_percentage:.4f}\")\n",
        "\n",
        "            # Evaluate the model on validation data\n",
        "            validation_accuracy, validation_precision, validation_positive_prediction_percentage = evaluate_model(model, validation_dataloader)\n",
        "            print(f\"Validation Accuracy: {validation_accuracy:.4f}, Precision: {validation_precision:.4f}, Positive Predictions: {validation_positive_prediction_percentage:.4f}\")\n",
        "\n",
        "    # Save the model weights\n",
        "    torch.save(model.state_dict(), f'model_{i}.pth')\n"
      ],
      "metadata": {
        "id": "iQoRf7ynYp7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ab52bef-f972-4454-8c51-5f4ed3e4f37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 1 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4128, Precision: 0.3732, Positive Predictions: 0.1402\n",
            "Validation Accuracy: 0.4221, Precision: 0.3144, Positive Predictions: 0.1022\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4221, Precision: 0.3514, Positive Predictions: 0.1549\n",
            "Validation Accuracy: 0.4105, Precision: 0.3016, Positive Predictions: 0.1617\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4425, Precision: 0.3693, Positive Predictions: 0.3634\n",
            "Validation Accuracy: 0.4142, Precision: 0.3540, Positive Predictions: 0.3297\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4087, Precision: 0.3564, Positive Predictions: 0.3925\n",
            "Validation Accuracy: 0.3879, Precision: 0.3266, Positive Predictions: 0.4101\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4676, Precision: 0.3677, Positive Predictions: 0.2569\n",
            "Validation Accuracy: 0.4013, Precision: 0.3691, Positive Predictions: 0.3504\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4816, Precision: 0.3865, Positive Predictions: 0.2087\n",
            "Validation Accuracy: 0.4152, Precision: 0.3543, Positive Predictions: 0.1934\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4487, Precision: 0.3817, Positive Predictions: 0.3293\n",
            "Validation Accuracy: 0.4744, Precision: 0.3708, Positive Predictions: 0.2655\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4712, Precision: 0.3885, Positive Predictions: 0.2810\n",
            "Validation Accuracy: 0.4655, Precision: 0.3657, Positive Predictions: 0.2537\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4422, Precision: 0.3743, Positive Predictions: 0.2980\n",
            "Validation Accuracy: 0.4365, Precision: 0.3588, Positive Predictions: 0.2403\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4322, Precision: 0.3927, Positive Predictions: 0.3590\n",
            "Validation Accuracy: 0.4330, Precision: 0.3698, Positive Predictions: 0.3833\n",
            "Training model 2 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.3978, Precision: 0.3903, Positive Predictions: 0.1964\n",
            "Validation Accuracy: 0.4302, Precision: 0.3667, Positive Predictions: 0.2039\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4250, Precision: 0.3620, Positive Predictions: 0.2253\n",
            "Validation Accuracy: 0.4157, Precision: 0.3143, Positive Predictions: 0.1935\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4432, Precision: 0.3590, Positive Predictions: 0.2818\n",
            "Validation Accuracy: 0.4129, Precision: 0.3410, Positive Predictions: 0.2571\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4117, Precision: 0.3573, Positive Predictions: 0.2534\n",
            "Validation Accuracy: 0.3952, Precision: 0.3295, Positive Predictions: 0.2423\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4691, Precision: 0.3757, Positive Predictions: 0.2407\n",
            "Validation Accuracy: 0.4009, Precision: 0.3719, Positive Predictions: 0.3385\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4630, Precision: 0.3466, Positive Predictions: 0.2569\n",
            "Validation Accuracy: 0.4017, Precision: 0.3346, Positive Predictions: 0.2892\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4508, Precision: 0.3881, Positive Predictions: 0.4351\n",
            "Validation Accuracy: 0.4668, Precision: 0.3716, Positive Predictions: 0.4130\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4713, Precision: 0.3743, Positive Predictions: 0.3645\n",
            "Validation Accuracy: 0.4551, Precision: 0.3584, Positive Predictions: 0.3268\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4409, Precision: 0.3772, Positive Predictions: 0.4375\n",
            "Validation Accuracy: 0.4178, Precision: 0.3577, Positive Predictions: 0.4336\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4336, Precision: 0.3913, Positive Predictions: 0.3405\n",
            "Validation Accuracy: 0.4366, Precision: 0.3643, Positive Predictions: 0.3165\n",
            "Training model 3 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.3985, Precision: 0.3783, Positive Predictions: 0.8299\n",
            "Validation Accuracy: 0.3701, Precision: 0.3486, Positive Predictions: 0.8608\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4261, Precision: 0.3659, Positive Predictions: 0.2242\n",
            "Validation Accuracy: 0.4231, Precision: 0.3298, Positive Predictions: 0.1722\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4299, Precision: 0.3509, Positive Predictions: 0.4567\n",
            "Validation Accuracy: 0.4030, Precision: 0.3356, Positive Predictions: 0.4324\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4132, Precision: 0.3632, Positive Predictions: 0.2919\n",
            "Validation Accuracy: 0.3966, Precision: 0.3335, Positive Predictions: 0.2449\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4709, Precision: 0.3870, Positive Predictions: 0.2952\n",
            "Validation Accuracy: 0.3969, Precision: 0.3768, Positive Predictions: 0.4085\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4850, Precision: 0.3911, Positive Predictions: 0.3074\n",
            "Validation Accuracy: 0.4156, Precision: 0.3646, Positive Predictions: 0.3400\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4503, Precision: 0.3723, Positive Predictions: 0.2193\n",
            "Validation Accuracy: 0.4670, Precision: 0.3537, Positive Predictions: 0.1801\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4748, Precision: 0.3863, Positive Predictions: 0.3087\n",
            "Validation Accuracy: 0.4649, Precision: 0.3641, Positive Predictions: 0.2788\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4457, Precision: 0.3852, Positive Predictions: 0.3855\n",
            "Validation Accuracy: 0.4349, Precision: 0.3695, Positive Predictions: 0.3594\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4289, Precision: 0.4006, Positive Predictions: 0.5985\n",
            "Validation Accuracy: 0.4284, Precision: 0.3790, Positive Predictions: 0.6258\n",
            "Training model 4 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4160, Precision: 0.3613, Positive Predictions: 0.2548\n",
            "Validation Accuracy: 0.4043, Precision: 0.3020, Positive Predictions: 0.2000\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4239, Precision: 0.3598, Positive Predictions: 0.3079\n",
            "Validation Accuracy: 0.4102, Precision: 0.3177, Positive Predictions: 0.2657\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4444, Precision: 0.3770, Positive Predictions: 0.4679\n",
            "Validation Accuracy: 0.4151, Precision: 0.3608, Positive Predictions: 0.4388\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4110, Precision: 0.3620, Positive Predictions: 0.2366\n",
            "Validation Accuracy: 0.4043, Precision: 0.3479, Positive Predictions: 0.1786\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4667, Precision: 0.3951, Positive Predictions: 0.4003\n",
            "Validation Accuracy: 0.3851, Precision: 0.3880, Positive Predictions: 0.6669\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4843, Precision: 0.3748, Positive Predictions: 0.3767\n",
            "Validation Accuracy: 0.4152, Precision: 0.3508, Positive Predictions: 0.4246\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4517, Precision: 0.3742, Positive Predictions: 0.1738\n",
            "Validation Accuracy: 0.4718, Precision: 0.3542, Positive Predictions: 0.1360\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4731, Precision: 0.3756, Positive Predictions: 0.2073\n",
            "Validation Accuracy: 0.4635, Precision: 0.3549, Positive Predictions: 0.2018\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4433, Precision: 0.3870, Positive Predictions: 0.3025\n",
            "Validation Accuracy: 0.4387, Precision: 0.3665, Positive Predictions: 0.2707\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4333, Precision: 0.3938, Positive Predictions: 0.2800\n",
            "Validation Accuracy: 0.4389, Precision: 0.3652, Positive Predictions: 0.2730\n",
            "Training model 5 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4062, Precision: 0.3331, Positive Predictions: 0.0555\n",
            "Validation Accuracy: 0.3840, Precision: 0.2891, Positive Predictions: 0.0393\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4254, Precision: 0.3684, Positive Predictions: 0.3584\n",
            "Validation Accuracy: 0.4168, Precision: 0.3326, Positive Predictions: 0.2877\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4426, Precision: 0.3861, Positive Predictions: 0.5941\n",
            "Validation Accuracy: 0.4119, Precision: 0.3738, Positive Predictions: 0.6061\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4109, Precision: 0.3615, Positive Predictions: 0.3636\n",
            "Validation Accuracy: 0.3983, Precision: 0.3356, Positive Predictions: 0.3288\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4703, Precision: 0.3773, Positive Predictions: 0.2848\n",
            "Validation Accuracy: 0.4000, Precision: 0.3711, Positive Predictions: 0.3913\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4838, Precision: 0.3755, Positive Predictions: 0.2456\n",
            "Validation Accuracy: 0.4160, Precision: 0.3519, Positive Predictions: 0.2437\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4510, Precision: 0.3894, Positive Predictions: 0.4377\n",
            "Validation Accuracy: 0.4708, Precision: 0.3768, Positive Predictions: 0.4010\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4766, Precision: 0.3865, Positive Predictions: 0.3606\n",
            "Validation Accuracy: 0.4631, Precision: 0.3646, Positive Predictions: 0.3365\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4390, Precision: 0.3907, Positive Predictions: 0.6322\n",
            "Validation Accuracy: 0.4243, Precision: 0.3785, Positive Predictions: 0.6753\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4331, Precision: 0.3887, Positive Predictions: 0.4257\n",
            "Validation Accuracy: 0.4278, Precision: 0.3652, Positive Predictions: 0.4549\n",
            "Training model 6 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4141, Precision: 0.3668, Positive Predictions: 0.1792\n",
            "Validation Accuracy: 0.4106, Precision: 0.3082, Positive Predictions: 0.1431\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4193, Precision: 0.3828, Positive Predictions: 0.2000\n",
            "Validation Accuracy: 0.4305, Precision: 0.3580, Positive Predictions: 0.1296\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4462, Precision: 0.3745, Positive Predictions: 0.3925\n",
            "Validation Accuracy: 0.4169, Precision: 0.3542, Positive Predictions: 0.3293\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4035, Precision: 0.3857, Positive Predictions: 0.6733\n",
            "Validation Accuracy: 0.3930, Precision: 0.3703, Positive Predictions: 0.5836\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4643, Precision: 0.3955, Positive Predictions: 0.2238\n",
            "Validation Accuracy: 0.3876, Precision: 0.3829, Positive Predictions: 0.3033\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4856, Precision: 0.3849, Positive Predictions: 0.3081\n",
            "Validation Accuracy: 0.4117, Precision: 0.3672, Positive Predictions: 0.3763\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4537, Precision: 0.3877, Positive Predictions: 0.2620\n",
            "Validation Accuracy: 0.4711, Precision: 0.3706, Positive Predictions: 0.2213\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4740, Precision: 0.3890, Positive Predictions: 0.3329\n",
            "Validation Accuracy: 0.4638, Precision: 0.3674, Positive Predictions: 0.2998\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4424, Precision: 0.3769, Positive Predictions: 0.4274\n",
            "Validation Accuracy: 0.4258, Precision: 0.3580, Positive Predictions: 0.3968\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4318, Precision: 0.3940, Positive Predictions: 0.5149\n",
            "Validation Accuracy: 0.4261, Precision: 0.3731, Positive Predictions: 0.5633\n",
            "Training model 7 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4063, Precision: 0.3708, Positive Predictions: 0.0976\n",
            "Validation Accuracy: 0.4285, Precision: 0.3256, Positive Predictions: 0.0819\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4233, Precision: 0.3503, Positive Predictions: 0.1486\n",
            "Validation Accuracy: 0.4133, Precision: 0.3026, Positive Predictions: 0.1541\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4400, Precision: 0.3516, Positive Predictions: 0.2450\n",
            "Validation Accuracy: 0.4141, Precision: 0.3391, Positive Predictions: 0.2316\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4117, Precision: 0.3744, Positive Predictions: 0.3090\n",
            "Validation Accuracy: 0.4044, Precision: 0.3504, Positive Predictions: 0.2222\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4639, Precision: 0.3663, Positive Predictions: 0.4055\n",
            "Validation Accuracy: 0.3979, Precision: 0.3680, Positive Predictions: 0.5218\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4857, Precision: 0.3847, Positive Predictions: 0.3638\n",
            "Validation Accuracy: 0.4158, Precision: 0.3593, Positive Predictions: 0.4069\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4497, Precision: 0.3748, Positive Predictions: 0.2752\n",
            "Validation Accuracy: 0.4638, Precision: 0.3530, Positive Predictions: 0.2522\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4744, Precision: 0.3805, Positive Predictions: 0.2621\n",
            "Validation Accuracy: 0.4654, Precision: 0.3596, Positive Predictions: 0.2384\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4453, Precision: 0.3854, Positive Predictions: 0.4458\n",
            "Validation Accuracy: 0.4295, Precision: 0.3684, Positive Predictions: 0.4523\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4266, Precision: 0.3857, Positive Predictions: 0.5578\n",
            "Validation Accuracy: 0.4151, Precision: 0.3642, Positive Predictions: 0.6113\n",
            "Training model 8 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4089, Precision: 0.3790, Positive Predictions: 0.2087\n",
            "Validation Accuracy: 0.4276, Precision: 0.3499, Positive Predictions: 0.2170\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4273, Precision: 0.3745, Positive Predictions: 0.2702\n",
            "Validation Accuracy: 0.4235, Precision: 0.3417, Positive Predictions: 0.2111\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4453, Precision: 0.3875, Positive Predictions: 0.4164\n",
            "Validation Accuracy: 0.4166, Precision: 0.3695, Positive Predictions: 0.3753\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4136, Precision: 0.3741, Positive Predictions: 0.3296\n",
            "Validation Accuracy: 0.4063, Precision: 0.3525, Positive Predictions: 0.2319\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4688, Precision: 0.3760, Positive Predictions: 0.1585\n",
            "Validation Accuracy: 0.4010, Precision: 0.3709, Positive Predictions: 0.2260\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4848, Precision: 0.3799, Positive Predictions: 0.2466\n",
            "Validation Accuracy: 0.4171, Precision: 0.3514, Positive Predictions: 0.2624\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4478, Precision: 0.3646, Positive Predictions: 0.2481\n",
            "Validation Accuracy: 0.4653, Precision: 0.3478, Positive Predictions: 0.2050\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4725, Precision: 0.3760, Positive Predictions: 0.2271\n",
            "Validation Accuracy: 0.4640, Precision: 0.3562, Positive Predictions: 0.2212\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4442, Precision: 0.3851, Positive Predictions: 0.4124\n",
            "Validation Accuracy: 0.4302, Precision: 0.3702, Positive Predictions: 0.4424\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4329, Precision: 0.3887, Positive Predictions: 0.4236\n",
            "Validation Accuracy: 0.4298, Precision: 0.3673, Positive Predictions: 0.4605\n",
            "Training model 9 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4061, Precision: 0.3716, Positive Predictions: 0.1429\n",
            "Validation Accuracy: 0.4219, Precision: 0.3401, Positive Predictions: 0.1208\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4254, Precision: 0.3734, Positive Predictions: 0.2951\n",
            "Validation Accuracy: 0.4238, Precision: 0.3434, Positive Predictions: 0.2252\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4344, Precision: 0.3979, Positive Predictions: 0.6194\n",
            "Validation Accuracy: 0.4105, Precision: 0.3905, Positive Predictions: 0.5615\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4129, Precision: 0.3714, Positive Predictions: 0.3562\n",
            "Validation Accuracy: 0.4029, Precision: 0.3453, Positive Predictions: 0.2831\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4693, Precision: 0.3841, Positive Predictions: 0.2115\n",
            "Validation Accuracy: 0.3973, Precision: 0.3758, Positive Predictions: 0.3049\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4849, Precision: 0.3833, Positive Predictions: 0.2196\n",
            "Validation Accuracy: 0.4159, Precision: 0.3523, Positive Predictions: 0.2329\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4535, Precision: 0.3757, Positive Predictions: 0.2874\n",
            "Validation Accuracy: 0.4694, Precision: 0.3588, Positive Predictions: 0.2463\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4745, Precision: 0.3873, Positive Predictions: 0.3181\n",
            "Validation Accuracy: 0.4655, Precision: 0.3669, Positive Predictions: 0.2909\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4465, Precision: 0.3908, Positive Predictions: 0.3602\n",
            "Validation Accuracy: 0.4348, Precision: 0.3725, Positive Predictions: 0.3709\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4327, Precision: 0.3846, Positive Predictions: 0.3343\n",
            "Validation Accuracy: 0.4304, Precision: 0.3532, Positive Predictions: 0.3180\n",
            "Training model 10 of 10\n",
            "Starting epoch 1 of 1\n",
            "mini_epoch [1/10]\n",
            "Training Accuracy: 0.4152, Precision: 0.3547, Positive Predictions: 0.1358\n",
            "Validation Accuracy: 0.4141, Precision: 0.2804, Positive Predictions: 0.1014\n",
            "mini_epoch [2/10]\n",
            "Training Accuracy: 0.4237, Precision: 0.3536, Positive Predictions: 0.2486\n",
            "Validation Accuracy: 0.4041, Precision: 0.3035, Positive Predictions: 0.2367\n",
            "mini_epoch [3/10]\n",
            "Training Accuracy: 0.4326, Precision: 0.3294, Positive Predictions: 0.1627\n",
            "Validation Accuracy: 0.4086, Precision: 0.3336, Positive Predictions: 0.1887\n",
            "mini_epoch [4/10]\n",
            "Training Accuracy: 0.4131, Precision: 0.3728, Positive Predictions: 0.1757\n",
            "Validation Accuracy: 0.4029, Precision: 0.3449, Positive Predictions: 0.1470\n",
            "mini_epoch [5/10]\n",
            "Training Accuracy: 0.4672, Precision: 0.3645, Positive Predictions: 0.2097\n",
            "Validation Accuracy: 0.4005, Precision: 0.3664, Positive Predictions: 0.2771\n",
            "mini_epoch [6/10]\n",
            "Training Accuracy: 0.4863, Precision: 0.3808, Positive Predictions: 0.2978\n",
            "Validation Accuracy: 0.4140, Precision: 0.3494, Positive Predictions: 0.3057\n",
            "mini_epoch [7/10]\n",
            "Training Accuracy: 0.4468, Precision: 0.3735, Positive Predictions: 0.2243\n",
            "Validation Accuracy: 0.4748, Precision: 0.3667, Positive Predictions: 0.1751\n",
            "mini_epoch [8/10]\n",
            "Training Accuracy: 0.4718, Precision: 0.3869, Positive Predictions: 0.4221\n",
            "Validation Accuracy: 0.4629, Precision: 0.3700, Positive Predictions: 0.4145\n",
            "mini_epoch [9/10]\n",
            "Training Accuracy: 0.4443, Precision: 0.3891, Positive Predictions: 0.3587\n",
            "Validation Accuracy: 0.4317, Precision: 0.3747, Positive Predictions: 0.3855\n",
            "mini_epoch [10/10]\n",
            "Training Accuracy: 0.4257, Precision: 0.4018, Positive Predictions: 0.3613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4319, Precision: 0.3798, Positive Predictions: 0.4115\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-99-3e437d0ee47b>\", line 40, in <module>\n",
            "    torch.save(model.state_dict(), f'model_{i}.pth')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 422, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 309, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 287, in __init__\n",
            "    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))\n",
            "RuntimeError: File model_9.pth cannot be opened.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def voting(models, dataloader):\n",
        "    all_preds = []\n",
        "    \n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for features, _ in dataloader:\n",
        "                features = features.to(device)\n",
        "                outputs = model(features)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                preds.extend(predicted.cpu().numpy())\n",
        "                \n",
        "        all_preds.append(preds)\n",
        "    \n",
        "    return np.array(all_preds)\n",
        "\n",
        "def blending(models, dataloader):\n",
        "    all_outputs = []\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, _ in dataloader:\n",
        "                features = features.to(device)\n",
        "                outputs = model(features)\n",
        "                outputs_list.extend(outputs.cpu().numpy())\n",
        "\n",
        "        all_outputs.append(outputs_list)\n",
        "\n",
        "    return np.mean(all_outputs, axis=0)\n",
        "\n",
        "datasets = {'Training': train_dataloader, 'Validation': validation_dataloader}\n",
        "\n",
        "for dataset_name, dataloader in datasets.items():\n",
        "    print(f\"Evaluating on {dataset_name} set:\")\n",
        "    # Voting\n",
        "    voting_preds = voting(models, dataloader)\n",
        "    voting_accuracy = accuracy_score(np.argmax(dataloader.dataset.data_Y, axis=1), np.argmax(np.mean(voting_preds, axis=0), axis=1))\n",
        "    voting_precision = precision_score(np.argmax(dataloader.dataset.data_Y, axis=1), np.argmax(np.mean(voting_preds, axis=0), axis=1), average='weighted')\n",
        "    \n",
        "    print(f\"Voting - Accuracy: {voting_accuracy:.4f}, Precision: {voting_precision:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3pkuY6IEWLSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blending\n",
        "blending_outputs = blending(models, dataloader)\n",
        "blending_accuracy = accuracy_score(np.argmax(dataloader.dataset.data_Y, axis=1), np.argmax(blending_outputs, axis=1))\n",
        "blending_precision = precision_score(np.argmax(dataloader.dataset.data_Y, axis=1), np.argmax(blending_outputs, axis=1), average='weighted')\n",
        "\n",
        "print(f\"Blending - Accuracy: {blending_accuracy:.4f}, Precision: {blending_precision:.4f}\")\n",
        "\n",
        "# AdaBoost\n",
        "X = np.vstack([features.numpy() for features, _ in dataloader])\n",
        "y = np.argmax(np.vstack([labels.numpy() for _, labels in dataloader]), axis=1)\n",
        "\n",
        "# Create a list of weak classifiers\n",
        "weak_classifiers = []\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    weak_classifier = lambda x: torch.max(model(torch.tensor(x, dtype=torch.float32).to(device)).cpu(), 1)[1].numpy()\n",
        "    weak_classifiers.append(weak_classifier)\n",
        "\n",
        "# Train the AdaBoost classifier\n",
        "adaboost = AdaBoostClassifier(base_estimator=None, n_estimators=len(weak_classifiers), algorithm='SAMME.R')\n",
        "adaboost.fit(X, y, sample_weight=None)\n",
        "\n",
        "# Evaluate the AdaBoost classifier\n",
        "adaboost_preds = adaboost.predict(X)\n",
        "adaboost_accuracy = accuracy_score(y, adaboost_preds)\n",
        "adaboost_precision = precision_score(y, adaboost_preds, average='weighted')\n",
        "\n",
        "print(f\"AdaBoost - Accuracy: {adaboost_accuracy:.4f}, Precision: {adaboost_precision:.4f}\")"
      ],
      "metadata": {
        "id": "IXwZ4-Z8XGR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task6"
      ],
      "metadata": {
        "id": "wnP5_Q5QXiRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for the test sets\n",
        "test_dataset_1 = StockDataset(test1['data_X'], test1['data_Y'], window_size=100)\n",
        "test_dataloader_1 = DataLoader(test_dataset_1, batch_size=1024, shuffle=True)\n",
        "\n",
        "test_dataset_2 = StockDataset(test2['data_X'], test2['data_Y'], window_size=100)\n",
        "test_dataloader_2 = DataLoader(test_dataset_2, batch_size=1024, shuffle=True)\n",
        "\n",
        "# Load the best model from Task 5\n",
        "best_model = StockPredictionModel(input_size, hidden_size, num_layers, num_classes)\n",
        "best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "best_model.to(device)\n",
        "\n",
        "# Evaluate the model on the test sets\n",
        "datasets = {'Test Set 1': test_dataloader_1, 'Test Set 2': test_dataloader_2}\n",
        "\n",
        "for dataset_name, dataloader in datasets.items():\n",
        "    print(f\"Evaluating on {dataset_name}:\")\n",
        "    accuracy, precision, positive_prediction_percentage = evaluate_model(best_model, dataloader)\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Positive Predictions: {positive_prediction_percentage:.4f}\")\n"
      ],
      "metadata": {
        "id": "BVFtg9uaXh2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}